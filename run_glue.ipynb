{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Finetuning the library models for sequence classification on GLUE.\"\"\"\n",
    "# You can also adapt this script on your own text classification task. Pointers for this are left as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needded classes from HuggingFace transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    TFGPTJForSequenceClassification,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will error if the minimal version of Transformers is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_min_version(\"4.27.0.dev0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keys used by tokenizer to select text to be tokenized<br>\n",
    "Data set used cola.<br>\n",
    "Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"cola\": (\"sentence\", None),\n",
    "}\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "region Command-line arguments<br>\n",
    "A few arguments with default values from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataArgs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __init__(self):\n",
    "    self.task_name = \"cola\"\n",
    "    self.precision = \"bfloat16\"\n",
    "    self.intra_op_parallelism_threads=56\n",
    "    self.inter_op_parallelism_threads=2\n",
    "    self.max_seq_length=128\n",
    "    self.checkpoint_save_freq = 500\n",
    "    self.overwrite_cache=True\n",
    "    self.max_train_samples=None\n",
    "    self.max_eval_samples=None\n",
    "    self.max_predict_samples=12\n",
    "    self.output_dir =\"./output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __init__(self):\n",
    "    self.model_name_or_path = \"EleutherAI/gpt-j-6B\"\n",
    "    self.cache_dir=None\n",
    "    self.model_revision=\"main\"\n",
    "    self.steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArgs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __init__(self):\n",
    "    self.local_rank =-1\n",
    "    self.seed =77\n",
    "    self.num_replicas_in_sync=1\n",
    "    self.per_device_train_batch_size=64\n",
    "    self.per_device_eval_batch_size=64\n",
    "    self.do_train=True\n",
    "    self.do_predict=True\n",
    "    self.do_eval=True\n",
    "    self.num_train_epochs=1.0\n",
    "    self.learning_rate=5e-06\n",
    "    self.output_dir =\"./output\"\n",
    "    self.xla =False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument parsing<br>\n",
    "See all possible arguments in src/transformers/training_args.py<br>\n",
    "or by passing the --help flag to this script.<br>\n",
    "We now keep distinct sets of args, for a cleaner separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataArgs()\n",
    "model_args = ModelArgs()\n",
    "train_args = TrainingArgs()\n",
    "training_args=train_args\n",
    "metric = evaluate.load(\"glue\", data_args.task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and thread settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_args.precision == \"bfloat16\" :\n",
    "  tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "elif data_args.precision == \"fp16\" :\n",
    "  tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "tf.config.threading.set_inter_op_parallelism_threads(data_args.inter_op_parallelism_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(data_args.intra_op_parallelism_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting for region Logging and transformer verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the verbosity to info of the Transformers logger (on main process only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_main_process(training_args.local_rank):\n",
    "   transformers.utils.logging.set_verbosity_info()\n",
    "   transformers.utils.logging.enable_default_handler()\n",
    "   transformers.utils.logging.enable_explicit_format()\n",
    "   logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and labels<br>\n",
    "Set seed before initializing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading and loading a dataset from the hub. In distributed training, the load_dataset function guarantee<br>\n",
    "that only one local process can concurrently download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    \"glue\",\n",
    "    data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset schema and Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets)\n",
    "print(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model config and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gpt2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gpt2\" if model_args.model_name_or_path == \"EleutherAI/gpt-j-6B\" else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=True,\n",
    "    revision=model_args.model_revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add special tokens for padding as GPT does not have a padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "config.pad_token_id=0\n",
    "sentence1_key, sentence2_key = task_to_keys[data_args.task_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ome models have set the order of the labels to use, so let's make sure we do use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = None\n",
    "config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "print(\"  Label to ID :\", config.label2id)\n",
    "print(\"  ID to Label :\", config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the tokenizer process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "       (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=False, max_length=max_seq_length, truncation=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize dataset and set a DataColltor for batching and any padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = raw_datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to convert raw dataset to tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_Dataset(datasets):\n",
    "    # Convert data to a tf.data.Dataset\n",
    "    dataset_options = tf.data.Options()\n",
    "    dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    num_replicas = -1 #training_args.strategy.num_replicas_in_sync\n",
    "    tf_data = {}\n",
    "    max_samples = {\n",
    "            \"train\": data_args.max_train_samples,\n",
    "            \"validation\": data_args.max_eval_samples,\n",
    "            \"test\": data_args.max_predict_samples,\n",
    "    }\n",
    "    num_replicas=1\n",
    "    for key in datasets.keys():\n",
    "        if key == \"train\" or key.startswith(\"validation\"):\n",
    "            assert \"label\" in datasets[key].features, f\"Missing labels from {key} data!\"\n",
    "        if key == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = training_args.per_device_train_batch_size * num_replicas\n",
    "        else:\n",
    "            shuffle = False\n",
    "            batch_size = training_args.per_device_eval_batch_size * num_replicas\n",
    "        samples_limit = max_samples[key]\n",
    "        dataset = datasets[key]\n",
    "        if samples_limit is not None:\n",
    "            dataset = dataset.select(range(samples_limit))\n",
    "\n",
    "        # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n",
    "        # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n",
    "        # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n",
    "        # yourself if you use this method, whereas they are automatically inferred from the model input names when\n",
    "        # using model.prepare_tf_dataset()\n",
    "        # For more info see the docs:\n",
    "        data = model.prepare_tf_dataset(\n",
    "                dataset,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "        )\n",
    "        data = data.with_options(dataset_options)\n",
    "        tf_data[key] = data\n",
    "    return tf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility fn to compute total number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_train_steps(tf_data):\n",
    "    if training_args.do_train:\n",
    "        if model_args.steps:\n",
    "            num_train_steps = model_args.steps\n",
    "            if num_train_steps > int(len(tf_data[\"train\"])) :\n",
    "                # for single epoch\n",
    "                num_train_steps = int(len(tf_data[\"train\"]))\n",
    "        else :\n",
    "            num_train_steps = len(tf_data[\"train\"]) * training_args.num_train_epochs\n",
    "    return num_train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to define Adam optimizer with Polynomialdecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer_with_decay(num_train_steps):\n",
    "    end_lr = (training_args.learning_rate)/np.sqrt(num_train_steps)\n",
    "    lr_scheduler = PolynomialDecay(\n",
    "        initial_learning_rate=training_args.learning_rate,\n",
    "        end_learning_rate=end_lr, decay_steps=num_train_steps\n",
    "    )\n",
    "    opt = Adam(learning_rate=lr_scheduler)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call back for checkpointing if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    callbacks = []\n",
    "    checkpoint=None\n",
    "    if (checkpoint) :\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "           filepath=training_args.output_dir,\n",
    "           save_weights_only=True,\n",
    "           monitor='accuracy',\n",
    "           mode='max',\n",
    "           save_freq=data_args.checkpoint_save_freq,\n",
    "           save_best_only=True,\n",
    "        )\n",
    "        callbacks.append(checkpoint_callback)\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main steps<br>\n",
    "Load the model : use name and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPTJForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = convert_to_tf_Dataset(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Optimizer,  and loss and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = compute_num_train_steps(tf_data)\n",
    "optimizer= adam_optimizer_with_decay(num_train_steps)\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"], jit_compile=training_args.xla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the mode : Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks= get_callbacks()\n",
    "steps_pe = int(len(tf_data[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    tf_data[\"train\"],\n",
    "    validation_data=tf_data[\"validation\"],\n",
    "    epochs=int(training_args.num_train_epochs),\n",
    "    steps_per_epoch=steps_pe,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.output_dir :\n",
    "    # If we're not pushing to hub, at least save a local copy when we're done\n",
    "    print(\"Save the model id dir :\",training_args.output_dir)\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "# Also let us reload from the saved dir\n",
    "model = TFGPTJForSequenceClassification.from_pretrained(\n",
    "            #model_args.model_name_or_path,\n",
    "            training_args.output_dir,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    ")\n",
    "# Show results for test\n",
    "# Show results for test\n",
    "def show_results(class_preds, key):\n",
    "    for i in range(7):\n",
    "      pred = int(class_preds[i])\n",
    "      pred_label = config.id2label[pred]\n",
    "      if data_args.task_name != 'mrpc':\n",
    "        print(f\"Sentence : {raw_datasets[key][i]['sentence']} : {pred_label}\")\n",
    "      else:\n",
    "        sent = raw_datasets[key][i]['sentence1'] + \" : \" + raw_datasets[key][i]['sentence2']\n",
    "        print(f\"Sentences : {sent} : {pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_predict(model, tf_data, key):\n",
    "    print(\"====================\",key, \"=========================\")\n",
    "    preds = model.predict(tf_data[key])[\"logits\"]\n",
    "    print(\" Done predictions:..\")\n",
    "    class_preds = tf.math.argmax(preds, axis=1)\n",
    "    if key != \"test\":\n",
    "      print(f\"{key} Accuracy :\", accuracy_score(class_preds,raw_datasets[key][\"label\"]))\n",
    "      print(metric.compute(predictions=class_preds, references=raw_datasets[key][\"label\"]))\n",
    "    else :\n",
    "      show_results(class_preds, key)\n",
    "    print(\"===================\", key, \" done.==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "al_predict(model, tf_data, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predict(model, tf_data, \"test\")\n",
    "print(\"===================Done.==================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
